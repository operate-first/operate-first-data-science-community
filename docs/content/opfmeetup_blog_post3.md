_This blog was generated by AI_

## Build your own JupyterHub image with AICoE-CI
### Date: 2021-12-14

### Speaker: Karanraj Chauhan

Speaker0    00:00:03    Cool. So, welcome everybody to another operate first data science community meetup. Today, before handing over to Karan to give his talk, I just want to give a quick, intro to the community so that everybody, especially new folks have a idea of what this meetup is all about. So hold on, let me share my screen and quickly go over it. Great. So can everybody see a big red screen? Excellent. So just quickly to introduce myself, my name is Michael Clifford. I'm a data scientist and data science manager working at Red Hat at the office of the CTO and working on among other things, the operate first project. So just for those who are unaware, like what is operate first? Operate first is really a large project that goes beyond strictly data science related topics and is focused on bringing greater transparency and kind of like the open source ethos to the operations of services. And it's about addressing an existing gap in the kind of software development life cycle, where the actual operating of an application is not if efficiently proved out by the developers and the kind of the knowledge of how to operate these applications, the operational experience not really exist in any traditional open source format. So by providing an open production community cloud, where we operate our software first, like operate first, we're able to of extend the open source software development process to include operating in the cloud. So what does that have to do with data science? Right. So, data science is actually like a critical to this initiative in a couple of ways. So the first reason is that like the initial service that we are operating is the open data hub, which is an open source cloud native platform that's designed for developing data science projects. So it's a large project in its own right. And I encourage all of you to check it out. But you can actually go ahead and start using it today as it's made available through operate first . In addition to that, many people have claimed that data science represents one of the most important workloads for our all services for the foreseeable future. So by using this platform for a wide range of data science projects, we can actually contribute to how these data science services are operated as well as to develop what some of the best practices are gonna be for cloud native data science development, which is still a relatively new discipline. And in addition to just using this platform, something that's equally as important, is that we want to actually use our kind of data science clients know how to contribute back to it. So service operations are growing more and more complicated and we believe that they can be better supported through the integration of machine learning tools. And then, so this practice is often referred to as AIOps and since the operate first cluster will be public infrastructure generating public data. It represents one of the first publicly available sources of data sets for AIOps that can be used to really push this field forward. So just quickly to recap, right. this data science community contributes to operate first by generating real world machine learning workloads, defining some cloud native best practices, and also supporting the services through the implementation of new open source AIOps tools. And again, what does that mean for this meetup? Right. So kind of the goal of this meetup is really to educate and collaborate with anyone who's interested in learning more about how people are working towards democratizing the development of production level AI, and specifically with regard to cloud native development and kind of collectively investigate this question, how do we make cloud native data science more open, more accessible and more reliable? And my hope is that we can do this together by addressing the kind of multidisciplinary nature of the current state of AI development and cloud operations in this meetup, as we cover technical topics relevant to data scientists, software engineers, DevOps professionals and more importantly, kind of the intersection of, of all of their work in an open and public forum like we are doing today. So just kind of the introduction, I hope that was clear. If you wanna have a closer look at any of our work or you wanna get involved with the community, here's a number of links that you can go to and I'll post these in the chat after the talk. So thank you. And with that, I'll hand it over to Karan to give us today's talk.

Speaker3    00:06:37    All right. Thanks, Michael. Let me just start by sharing my screen. Can you let me know if you see a set of slides on your screens?  

Speaker0    00:06:52    I do.  

Speaker3    00:06:53    Okay, perfect. Yeah, so hello all. And thanks for tuning in to today's data science meetup. In this part, I wanted to talk about how you or any data scientist in the open source community can share their content with the rest of the world in a reproducible way using some of the tools developed in operate first. So, basically we're gonna go over first. what are some of the challenges that you might run into when trying to share your data science content. Then I'll describe the solution that we've been using in our team, using operate first thoth and AICoE-CI. Then I'll go through a step by step, how you can adopt this for your own projects. Then we'll have a short demo and then finally we'll leave some time for Q&A. All right. So, what are some of the pin points or what are some of the requirements that you should keep in mind when trying to share your content? Well, the first one and probably the most obvious one is how are the python dependencies of your project specified? So for example, if you've only specified, let's say TensorFlow without any versions in a requirements.txt, then it's gonna be a issue. If you have trained your models using TensorFlow 2.0, and I'm trying to run it with Tensorflow 1.0 or something like that. So first thing to address is python dependencies. Next is, are there any system libraries that your project depends on? So for example, are there any specific RPMs for like metrics multiplication or something like that, that are needed for your model to train or something like that? And what happens if I don't have those libraries? So that's the second thing to address. And lastly, what if your project needs any specific computer resources? For example, if you're using, let's say 32 gigabytes of memory to train your model, and if I don't have that, then it's gonna be hard to share your code with me. So here's how we basically address these things in our team. So first of for Python dependencies, we use pipfiles to specify dependencies and we pin them down using recommendations from the thoth recommendations system, then for making sure that we have the same build and the same libraries and everything. We package our entire, project GitHub repo into a container image using AICoE-CI. And that's gonna be pretty much the primary focus of today's presentation. And, and the main benefit we get is we can use AICoE-CI to automatically build these container images and then deploy them. And then finally to make sure that we, the creators of content and also the consumers of the content have the same computer resources. We use the cloud environment, which is Jupyter hub provided using the operate first cluster. So yeah, that's basically like a short summary of the solution that we use in our team to share our work. And, next we can go and look into each of these steps in detail and how you can use them for your own repositories. All right. So for Python dependencies, you can use the thamos tool, which is a command line tool developed by the thoth team. And using this tool, you can basically specify your dependencies by just saying, thamos and then your dependencies. So what that will do is, create a pipfile with all those dependencies, and any other SIM like small configuration bits into it. So once you have this pipfile, you can go ahead and say, Thamos advice, and that will basically send the, requirements stack to the thoth recommendation service, and then based on the submitted pipfile, it will advise you, or it will give you a recommendation on the stack that you should use in your project. So yeah, once you run this Thamos advice step, you should have a pipfile.lock created for you and with the best recommendations from the thoth recommendation system and it will look something like this where you'll have all the versions of all the dependencies pinned down, along with the indices and the hash as well. So that's the first step in making your content reproducible, which is to use a pipfile, hopefully with thoth recommendations. So that it's an ideal application stack. So the next thing we wanna do is create a project container image. So here the main goal is to basically package all the system libraries and requirements or any initial setup bits or things like that into a container image. And we can describe this image using S2i scripts or docker file and build this image using AICOE-CI. And the way it works is, basically every time you create a new release on your GitHub repository or you can also create a special issue on your GitHub repository, and that will automatically trigger these build pipelines to take the GitHub repo and package it into a container image and then push it to the quay repository. So if you want to use this and if you choose to create your GitHub repo inside, one of the, AICoE organizations, then AICoE-CI is already up and running. So it's a pretty easy setup to use it. But even if you wanna host your own instance of AICoE-CI, that can also be easily done. And, in either one of these cases, it takes like less than five steps or so to get up and running. So yeah, let's look into one of these steps. So the first thing you'd need to do is install this, GITHUB application, AICoE-CI into your own organization or your own repo. And again, if your repo happens to be inside AICoE organizations, then this is already up and running. The next thing you'd need to do is add sesheta as a collaborator, and such sesheta is basically a GitHub robot account that basically handles the issues for build creation and it comments once the image is built and so on and so forth. So it's also not a strict requirement, but it's a really nice luxury to have. And again, if your repo is on AICoE, then you don't need to do this step, then once you've added sesheta, the third thing you need to do is add this config file to your GitHub repo. And here you can basically specify where you want the image to be pushed once the image is built. And also you can specify how you want to build the image. So for this particular example, you can see that we've set the build strategy as source, which means we're trying to do a source to image build for this particular repository. And of course you can also specify the base image right in the config file as well. And if you want to build your image using a Docker file, you can do that as well, by just specifying the build strategy to be Docker file, and then mentioning where the Docker file exists within your repo, like the path to the Docker file. And yeah, that's pretty much it for the third step. And then once you've added this config file to your repo, the next part is to set up the, image repository on quay, where you want to push this image. So for repos within operate first or AICoE, for now, you can just open an issue on this particular repository with the template that says onboard project to AICoE and then basically our team will create a quay repo for you. And yeah, that should be it. Then you're all set to go. And in the case that you are running AICoE-CI on your own cluster, then you can, you can manually create a empty quay repository and then create a robot account on quay and give it access to that repository. And yeah, that's it for this step. And once the robot account is created, you can share the pull secret for that robot account with the, AICoE-CI instance. And yeah, that's pretty much it. So these are the five steps, and then once you have done these steps, you are all set to trigger these built pipelines. If you're using AICoE, then you don't need to manually add the secret. So, yeah, so once this is set up, what happens is every time you are gonna create a new release on your github repo, the bill pipelines will get triggered, and then it'll start building the container image from your github repo. And then once it's built it, it will push it to the quay repository. So that's the second step. And at the end of this step, you'll basically have an image that is ready to be shared with the rest of the world. So so once you have the image, how do you share it with others and make sure that you're running the same, compute environment. And so for that, we use that Jupyter hub that is deployed using operate first, which basically looks like this. So, if you're not familiar with jupyterhub, it's basically a service that provides a shared infrastructure within which you can run your Jupyter lab or your classic jupyter notebook environments, and work on your notebooks. So this is what it looks like, basically, it has like a list of images that you can spawn, as well as, any configurations for that container image. So for example, you can set one CPU core, and eight gigabytes of memory and then you can also set if you need any particular number of GPU or any environment variables and so-on and so-forth. So what we're gonna do next is basically take this image that AICoE-CI created, and then we're gonna add it to this Jupyterhub instance so that it can be accessed by anyone on the internet with the GitHub account. So, to add this to the Jupyterhub, what you need to do is create a ImageStream manifest like this. And here, the main thing to keep in mind is basically add the URL of the quay repository or any image repository where the image exists and also the github repository URL over here. So yeah, you can basically use the same image stream manifest, and then just replace the GitHub and quay URLs with your own, uh, URLs. And then once you've created this, you can add it to whatever Jupyterhub instance you want to make your image available on. So in this case, I wanted to make this image, available on the OS climate cluster. So, so I just made a pull request like this, adding that image stream to the specific Jupyter hub instance over here. And yeah, that's pretty much it, once this PR gets merged and then Jupyterhub is redeployed, you'll see that the particular project that you wanted to share becomes available right here. So then now anyone on the internet can go ahead and click on this particular, project and click on start server and then explore your data science content. So that was the last step in this process. And at the end of this, just to recap what we did so far, first we used the thoth recommended  pipfile to specify our dependencies, get the best possible application stack for our project. Then we set up AICoE-CI in our github repository, which was basically essentially two main steps, which is adding this config file to your repository, and then opening this issue to onboard that project onto AICoE-CI. And then once that was done, we just create a release on the GitHub repo or open a issue with the specific templete to build this image. And then once that image is built, we just add it to Jupyterhub through a PR and it's ready for anyone to explore. So, that was pretty much it, like that's a solution we used to share our content with the world. And, let me go through a very short demo, just to show what it looks like on an existing repo. So, this is a repo that we have for the OS climate project. And as you can see, we have mentioned our dependencies in a pipfile and then any library, like system library requirements over here, for example, for this particular project we needed the Java open JDK installed. So we've mentioned that in a docker file here, and then here in the config file for the AICoE-CI, we specify that we wanna build this image using a docker file and that it exists on the root of the repository. And also we specify where we want this image to be pushed once it's built by the built pipelines. And yeah, that's the only setup that we needed to do. And then once this is set up, we can go to the releases page and then try to draft a new release and then we can choose a new tag for this new release. Let's go with 13, let's specify the tag and then for now I'll just use the autogenerated release notes and then, yeah, that's pretty much it, you can go ahead and click on publish release, and yeah, once this release has been made, it should have triggered the build pipelines to start building the image for this repo. And we can check that on the Tekton dashboard and see if that works or not. So this is the Tekton that's deployed on the operate first. And I think this should be the pipeline that just got triggered, cuz it says 27 seconds ago, if you can click here, look at the repository, oh, where is the repository? I forget what tab it showed the repository name in, but basically in one of these tab it shows like what GitHub repo triggered this build and then you can track. oh, there it is. Repo name is the repo that we wanted and this is the tag which we wanted to use for our image. And as you can see, the build is currently building and yeah, once this is built, it'll take a couple of minutes to finish building the entire image, but once this is built, it will push this image to the quay repo as specified in the config file, which happens to be this repo right here. And then yeah, once it's on the quay repo and it's tagged as the latest image, it should reflect on the Jupyterhub over here. And then now, and anytime someone starts, this image they'll have all the project files and all the project dependencies and everything packed into this image and ready to run. So I think that was all for the demo. I don't wanna keep waiting for this build to finish, because it might take a couple of minutes to finish. But for now we can trust me and imagine that I'm not lying. It actually works. But yeah, that's all for the demo. And just before I wrap it up, just wanted to mention it briefly. So if you don't want to go through the process of setting up AICoE-CI and you just want to quickly try out what these build pipelines might look like for your own repo. Then we also have a project called project meteor where you can just go to this link over here, and then drop the link to your GitHub repo and then click on this arrow to start building the image. And then it'll build the image and deploy it on a Jupyterhub that we have up and running. So of course this is not a complete, so you won't get all the configurations like you won't be able to decide where they image gets pushed or where it gets deployed and so on, but it's really nice if you wanna quickly try out what these build pipelines can do for you in a couple of minutes. So I think that was all that I wanted to talk about. And if you have any questions, now is the time

Speaker0    00:26:38    Awesome. Thanks Karan. Does anybody have any questions about any of that?

Speaker3    00:26:59    Cool. So I take No questions then. And of course I'll drop the link to the slides and everything in the agenda doc, I think as well. So if there is something that you missed or something, some link that you want that should be available over there. 

Speaker0    00:27:18    Cool. I mean, this is really cool and important stuff. I think, especially for like reproducibility in the way that we share all of our work and, and collaborate in our group. So that's great. And yeah, it's good to see how to go through the process. For anyone I just dropped a bunch of links in the chat. I said I'd share them from but kind of last slide that I'd shown just to make sure if anyone is not aware of all the ways to interact with the operate first data science community and take a look at those. But cool. So any other questions, topics, anything else anybody wants to bring up before we jump off? Cool. Well as always, thank you all so much for coming. Thank you so much, Karan, for going through that with us. And, this definitely will be the last meetup of the year. So we'll pick things back up after the new year. So with that, have a good break everyone. 
