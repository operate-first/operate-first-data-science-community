_This blog was generated by AI_

## Reproducibility & Dependencies for Jupyter Notebooks
### Date: 2021-11-30

### Speaker: Francesco Murdaca

Speaker1    00:00:02    Great. Welcome everyone to the second meeting of the operate first data science community meetup. Appreciate you all for joining today. Today we're going to have, Francesco, talk to us about reproducibility and dependencies for Jupyter notebooks. We'll get a chance to do a live tutorial on some different use cases that, he'll show us. Again, all of this will be on the operate first platform running the open data hub. Unless there's any questions at the top, Francesco, take it away.

Speaker2    00:00:54    Thanks, Michael. Thanks for inviting me. So let me share the screen. Please let me know if the presentation is good. Can you see now?

Speaker1    00:01:16    Yes.

Speaker2    00:01:17    Okay. So if it looks good, then I will start. So thank you. I'm very happy to be in this meetup. So what we are doing to see today, so we're gonna talk about Jupyter notebooks, a very quick introduction. I guess everyone use them. Then we are gonna focus on the problem that we're trying to solve. So specifically how we can make them reproducible, how we can help the creator of the notebooks and the user of the notebooks on working with this tutorials or this experiment that they create in Jupyter notebooks. Then we are gonna introduce project thoth, just an overview of it and what solution has been introduced in project thoth in order to solve this problem for reproducibility of Jupyter notebooks. And the last part is just an introduction to the actual tutorial that we are gonna do all together at the end of this presentation. If you have any question, please let me know at any moment. So let's start with the first section. So Jupyter notebooks, so everyone is familiar with Jupyter notebooks. They are open source web application. That is also actually an application that they introduced today, that you can install on your machines, but they're mostly known for working on the web and, you can use different type of, languages, so not just Python, but there are many other kernels that support many other programming languages. You can share this notebook also with someone else and we will see what is the problem when we need to share these notebooks. They are interactive. You can do a lot of things, plotting images. There are videos you can embed everything in this kind of notebooks and they can also leverage some big data, solution that you can directly use through your Jupyter notebooks. So they are used heavily by anyone I guess today. So not just data scientists, developers, educators, they're used in many fields and everywhere. So it's basically used everywhere. So what is the problem that we are trying to solve? So the Jupyter notebooks is, as you saw is basically giving you a notebook interactive that you can just write your code and run in your cell depending on the kernal that you're using. But there are some issues when you need to share this notebook, for example. And one of the main problem is the dependency. So let's have a look at this case, for example, I am in my notebook and I want to install this library. When I install this library for my implication, I get this opencv Python installed in the kernel, but, of course, when we install this package, we don't get just the direct dependency that everyone thinks or might think, but there are also all the other transitive dependencies that comes together with that package. So when you install TensorFlow or boto3 or whatever package, you get also other dependencies together with it. And, if you don't specify anything related to this package, if I install this opencv Python today or in one week, or even tomorrow could be that there is a new release of this package. And basically my notebook can break and nothing will work if I try to share this notebook with someone else. As this is actually the problem is what about the versions so when install, I install these packages, I get packages that, that have releases every day. So you see that each of these packages have a lot of releases and each day might have a new release that potentially could break the code. Doesn't have to be 1% sure, but it could break the code. And there also, as you see, they are quite often. So you see there is for example, once per month for numpy and each of them could break the dependencies and could break the notebook. So together with the version, there is also the problem of hashes. If you are worried about security, then you want to know also where or what kind of hashes are present with this package. So you want to know where it came from and from where it was installed. There is also some new solution today, like, six store, for example, but this is the problem of the securities is also connected to the fact that this hashes come from somewhere else. And what about the Python interpreter? So we just talking about the packages, but when I work, I also need to choose a specific pattern interpreter. So 3.4, 3.6, 3.10 today. So you can also change this kind of decision when you want to work on some specific python project. And so if we have a look at all the actually layers, we can see that there are many layers that could actually break your notebook. So starting from the dependency, you can have also the interpreter that can create problems. There are specific kernel module for some or wheels that needs to be installed on the operating system in order to work. Some dependency, for example, opencv Python requires some specificlibrary on the kernel in order to work. And this is something that not everyone knows or is aware. And especially if you want to share these notebooks, then we need to have a way to provide this information to someone else. And of course there's also the operating system and the hardware. So you can have any kind of operating system, fedora, RHEL, the different type of operating system and different version of these operating systems and then the hardware itself. So it could be, you are using CPU or GPU to work on your experiment. So all this kind of information are important if you want to guarantee reproducibility of the of the experiment and of the Jupyter notebooks. So these are some of the example that might cause these issues. So as you understood, from what we just saw in the previous slides, if I try to run this command today, or in one month, I can take different version, not just of the direct dependency, but also the transitive dependencies. And this means that anything in one of this three of the dependency can break my code. So this is not a solution when you want to work or when you want to guarantee the responsibility of your experiments. And yes, of course, this is not gonna guarantee reproductibility. This is another example. Many notebooks are provided with requirements.txt file and adding just a list of the packages, similar to what we just saw. So this is also not going to guarantee reproducibility, because there is no mention of the direct dependencies versions and there is no information related to the transit one, no hashes, you don't have this kind of information. So this cannot guarantee reproducibility. So if we just focus on this problem. So Jupyter notebooks by default are not standalone. So when you want to give a notebook to someone, then you need to also provide with this commands, for example, in the cells that basically are emulating what you give in a requirement.txt file on any file for the dependencies. And so the first problem is not just, the dependency are missing, but the Jupyter notebooks are also not facilitating that because we need to provide always a Jupyter notebook and some files to with the information related to the dependencies. This is also a problem when you want to containerize the notebooks, because you need also to have these files that always comes with it. And of course, when we need to share, as we mentioned, I need to give this information to someone else. And if I forget to send also the requirement.txt file, then the other one, the other person cannot reproduce this experiment. And of course, we talk about the reproducibility in terms of just the dependencies at this level, because if you want to reproduce experiments, you need to consider many other things. But here we are talking about the dependency itself. So one specific piece in the general reproducibility problem of experiments, but an important one, if you want to at least run the notebooks. So let's see some of the difficulties that authors can have, and the consumers can have. So the, the first thing, the author needs to create the environment, and then it needs to, oh, I need to give this environment also to someone else. So in theory, I need to update every time, the dependencies that I create. And I'm not talking about these commands in the cells, which as we mentioned, are not giving us any reproducibility, but also if we use manifest, this could be also an hassle a bit because they have to maintain them. And, if I forget to update one, then this can also cause issues when I give this dependencies to someone else. And so what we want to focus is having a way to help the developer, the scientist to support them in this task. So we want to facilitate this task that are in any case, something that needs to be done when you want to work with the Jupyter notebooks. And if we have a way to guarantee this, then we can also guarantee that this notebooks will be reproducible when we will share them. So now I will move a little bit on the thoth project itself in general. So project thoth wants to have a service that basically helps developers, data scientists to have educated decision when they need to select the dependencies. So I want to work on a specific AIML project. What kind of dependencies should I use. And, should I use Pytorch?, Should I use TensorFlow? what version of numpy? So all this kind of information that theoretically there is no knowledge, unless there is someone that knows we want to provide the service that is automatically providing this information. And other two goals are to deliver optimize secured and well maintain images for your application. So if you want to work on computer vision, then thoth can provide you with container image that is optimized and secure just for that specific goal. And then we want to automate all of this. So the developers and the scientists should not focus on dependencies management even though they should be aware of these problems. In this way, with this kind of tool and services, this could be basically offloaded from them. So they will focus on the real problems and the rest can be given to bots that can automate the dependency resolution, the creation of images, and let me go ahead. So when we talk about the educated decision, we mean that you can actually select what type of stocks or dependency you want, you are interested in. So I'm working with on a specific project and I'm interested just in latest, but I could also be interested maybe in creating deployment on production. And I don't want any specific packages that could cause issue in terms of security. So in that case, I would ask the recommended system just to give me a software stack on security, or maybe I'm interested in performance. So I want to have the most performance software stack for training my model. I don't care about security in that case, but I want to have the most performance software stack. And so you have basically a way to make a decision on what type of recommendation, not just, I want this version of this package, but I could also select why I'm taking this version. What we observed is we have a lot of knowledge regarding the software stacks in the Python ecosystem. So we know if a package can be installed in a specific environment we know the dependencies that are installed, and we know if there are any kind of performance difference when you run a specific code we have information about API. So we know that package could be installed, but is not able maybe to run because there is some API incompatibility. We have information about CVE analyzers, and we have also source code meta information as we call them because those information are coming from, maybe the community. So you're using some open source project, but you don't know that that project is not well maintained. They don't use CI, they don't use tests, but you're using it in your project. So this kind of information that are important when you want to work on a project. And this is something that I add and something that Fido introducing in our team. So the concept of these prescriptions, there is solution engine that I will link later about the resolver that is in thoth. And there is a video that Fido explain about the solution. And here you find also another video about this concept of prescription. So we want to have a way to heal Python applications, and this can be done from the community point of view. So you can basically create yamal file that have some specific pipeline units that can provide fix or specific information in terms of projects. And this could be integrated directly in the resolution engine, that is part of project thoth. And basically this allows to have a specific solution for specific projects. And if you want to know more about the prescription, have a look at the video that you will find in the slides. There is a video about prescription made by Fido and another important topic you might be interested in is also about how we provide this knowledge, how the service is providing the recommendation, and it uses reinforcement learning. And the video that I linked there is also made by Fido, and you will find all the description of the solution and how it works on the backend. So if you're interested in that you can have a look at this videos later. Integration, so thoth service is integrated in different developer tools. So you have command line, you have the bots that can provide this information. You have some source to image builds, you have optimizing deployment pipelines. And as we mentioned, what we want to provide is actually something for the Jupyter notebooks, which is the topic that we're gonna address today. So if you remember the problem that we just described in the previous slides, what thoth want to do is to help the developers, basically have the dependency managed by the resolution engine and have a way to share the notebooks in a very safe way. And that can guarantee reproducibility of the notebooks. And that's why we introduced package, which is called Jupyterlab requirements. So it's a Jupyterlab extension for dependency management. And when you basically select your dependencies in your notebooks, and we will do that together later in the tutorial, you will see that the dependency will be stored in the notebook metadata. So if you think about the problem of sending this notebook with all the other files, I need to remember to think that now with this kind of extension, you don't have to worry about it because everything will be stored in the metadata itself, even the resolution engine that was used the runtime environment that we mentioned before. So you don't want to know just the dependency, but also the interpreter that was used, the operating system, the hardware that was used running that notebook. So all this information will be stored in the metadata and the developer basically doesn't have to worry about that anymore. There are three ways to interact with our service, thoth service. And you can use basically the Jupyter magic commands. So something that you find indirectly in the Jupyter notebooks, you have a cli if you want to integrate Jupyter lab requirements in a pipeline or verify that the notebook is reproducible for example, or if you want to use that is also UI that provide basically another way to interact with the service. We were gonna see specifically this solution today. So we're gonna focus on the Jupyter magic commands and we will see that you can do many type of things. We can handle the dependency easily directly from the notebook sales. You can also handle the kernels. If you want to create them. You want to verify what packages are present in any kernel. And we can lock them. We can check if the notebook is reproducible or not, and you can also select the resolution engines if you want to use, for example, thoth or pipenv. Same functionalities provided with the CLI. So, as I said, this is just another way to interact with the with the service. And this is specifically done for the Jupyter notebooks. And then there is the UI, which might need some bit of love from some designer of UI, but the part is working and this is also some possibility if someone wants to use the UI and to try that you can just install it and on Jupyterlab. And we're gonna see that in a moment. So let's have a look at the tutorial itself. I will go quickly here because we can spend more time on the tutorial itself. We're gonna see four cases. So the first case is I have a new notebook and I want to start installing my dependencies and and be sure that this notebooks are reproducible if I want to give them to someone else. Then for example, you have some existing notebooks and I want to make them reproducible. Then there are some way to do that directly from the package. Then there is the other problem maybe some notebooks with the pip cell commands.And as we mentioned before, this is not something that can guarantee reproducibility. So we provide also way to convert those all notebooks with this new solution and also how to use a reproducible notebook. So if someone give me a reproducible notebook, how do I know that it's reproducible? And we will see how to check that. And with one command, you can be ready to work on the specific project. To do the tutorial, we're gonna use operate first of course, that everyone is aware and was presented also last time. And we're gonna use a project meteor, which is another project that was developed in the AICOE. Basically the idea is that we have a URL of a specific tutorial that you can insert in the UI, and this can create the environment for you. So you don't have to worry about anything. You just provide that you URL, and this will create two images, one for the Jupyter book. And basically there, we will see the tutorial described what the steps you're gonna make and that Jupiter hub environment that is where you basically can run the notebooks and run your code. This is the UI that we're gonna use. This is the UI related to the meteor itself. So this is already happening when we insert the URL. And as you see, there will be two images that will be created that you can find open as a Jupyterhub, open as a website, and once it's ready, this is how the tutorial will look like, and this is the environment that we're gonna use. So we can, I will just conclude through the slides and then we can move to the tutorial. So just to summarize, so what what we can do now with this kind of solution is that we don't have to worry about syncing the files or I have to remember that I was using that operating system. I need to tell that person now I have all this information stored in the notebook, so I can provide them directly this facilitator. So the build of images, because the dependency are embedded in the notebooks, you can just extract them and install them. And also the Jupyter notebook now can be shared as a standalone units, as we mentioned, and with a single click, you can just repeat the same experiment. So this was the focus of our talk. I hope you like and you got into the idea of why we are providing the solution. And if you're interested at anything that is done on project thoth, beside the video that you show in the other slides here, you have the link to the website, to the Twitter, to the YouTube videos. So if you're interested in more videos and content that we produce, and that is the link through the tutorial that you can do together today, or you are able to repeat it when you want. So with this, thank you. If you have any question, please, let me know.

Speaker1    00:24:22    Cool, thanks, Francesco. Jessica, any questions?

Speaker4    00:24:37    I had a small question about recommendation types. So I saw that you can different types of recommendations, like stable performance security, and so on. So I was wondering like, how is performance measured, like for different stacks, like, is performance, meaning like faster matrix multiplication, or like faster neural net training, or like, how do you measure that?

Speaker2    00:25:01    So there is another component that was not mentioned in the slides, which is called the ammo. So ammo is able to run specific script. You can provide the environment you want to run so dependencies operating system, and that will create a specific container with this requirements and will run that script in there in order to gather this performance. We have what we call the micro benchmarks. And we have a repository for that where we stayed. Yeah. Some of them are basically matrix multiplication based, but you can have different one. So we created the several of them and we run them in this API on ammo, and then this will be stored in our knowledge graph. And this will basically provide the difference in the performance on the stocks. So maybe tensorflow is faster on a specific script then if you want to do computer vision is better pytorch. If you want to do another project is better tensorflow. So this kind of information that we store.

Speaker4    00:26:08    Awesome. Makes sense. Thank you.

Speaker2    00:26:10    Thank you for the question.

Speaker1    00:26:17    Max. Go ahead.

Speaker5    00:26:18    Yeah. Thanks for the great presentation. You mentioned that thoth is based on reinforcement learning. Can you describe a little bit what's going on in terms of reinforcement learning?

Speaker2    00:26:32    This will be maybe another entire talk, but I cannot, I mean, I could summarize it. We have Monte carlo tree search as a basic algorithm, but there is a lot more and the resolution process have quite a complicated thing. So if I can provide you with the link or we can invite the Fido and we can make a presentation about the reinforcement learning algorithm use on thoth, but that will take quite, I think at least, another meetup to describe all of that, but I can give you the, the link to the video that Fido already made in this slide. And I don't think we have the time today because we need to go through the tutorial, but if you have more question, I can also point you to, where the documentation are and other things related to the algorithm.

Speaker1    00:27:27    Great. Thanks.

Speaker2    00:27:28    Thank you.

Speaker1    00:27:38    Cool. Any other questions? Great. So as far as the tutorial Francesco, are you gonna continue to share your screen and show folks how to get started or how do you wanna proceed?

Speaker2    00:27:54    All right, cool. So can you see my screen? So to speed up a bit, I already did the first step. So basically what we mentioned is meteor, where you can input any project. You can find the link in the chat, but for this tutorial specifically, I already ran it. So two images that we mentioned in the slide are already created, one is the Jupyter book that, we're gonna follow.

Speaker1    00:28:35    What is the repo link Francisco that people should use?

Speaker2    00:28:39    Yeah, I think it's not necessary to rerun all the meters in this case because we have, we created it already. If you want to try with the tutorial, you can just insert that URL that I pass pasted in the chat. And otherwise this is the link that I think you should also see the image here. The new feature that Tom introduced, you can see that you should have meteors created already, and you can just select the one that is called manage dependencies tutorial in 6696. I will post link in the chat. So if everyone is good, we, we should be able to see the Jupyter book where we describe all the things that we're gonna see today. And you can also open the Jupyterhub and this will lead us to the Jupyterlab environment. So we recently updated this image. So if we check the version of the jupyterlab requirements that we're gonna use, you should all get the versions 0.13.

Speaker1    00:30:13    Do you have a question?

Speaker5    00:30:15    Yeah. Sorry. The font is very small. I was wondering whether we can increase that a bit.

Speaker2    00:30:21    Sorry.

Speaker5    00:30:22    The font is very small. Maybe we can increase the font size.

Speaker2    00:30:25    Oh, yes. Sorry about that.

Speaker5    00:30:30    Great. Thank you.

Speaker2    00:30:31    Better.

Speaker5    00:30:32    Yes. Much better.

Speaker2    00:30:33    Perfect. Sorry. I have a very large screen. I think not see the difference when I present. So if everyone is good, we can go ahead. If we're having any problem, let me know, or just ask and we can go, we can go through them.

Speaker1    00:30:57    One point I think people might be sticking on, cause you were already signed in Francesco that people will get the spawner page and have to select the image with the correct meteor suffix.

Speaker2    00:31:13    Yes. So if we go here, Maybe we describe it or maybe not. Okay. If you are all in this same setting as I am there is one thing that we have to do actually, unfortunately. So if you go to the repo itself, we need to clone the repo. So it's the first thing, Jupyter lab,  GIT is the extension that is provided with the Elyra so we are able to go here and you can basically clone a repo. You can do it from the terminal, of course, but, this is way easier. You can just take this one. And you should be able own your image. You already selected the image. Maybe the medium size would've been been good. Maybe I can show that one moment. So I guess when you open this one you should be in this section, let's see when it starts, okay. So this should have been the one that you saw and you didn't see the image that we have to pick. It's called experimental notebook dependency image. And just to not have any trouble with the dependencies, maybe let's pick up the medium size and then we can just start the server. So meanwhile, we can just have a look at the tutorial Jupyterbook. So basically here we describe what you saw in the presentation. So what what we store in the notebooks, why we store them, what we are trying to do with this solution. And those are the four steps that we are gonna see today, or first of all, lets set up the environment. It's something that you don't have to do because everything is done automatically by meteor. So we can move directly to part two, which is managing the dependencies. And here we describe what we already saw in the presentation. So the three way to handle the dependency with this extension. And we will focus specifically on the magic commands. So let's have a look at the first example, which is starting working on a new notebook. So if you are all here and you already cloned the repo as we show before you can use this, you are all that type in the chat, and you should see the repo here. So this is basically the tutorial clone, and we can go to the notebook section and we can create a new notebook. So let's save a new notebook. As you see, there is nothing here. And if you try to run, %horus check, will get this table. And this table basically show you what is missing in order to guarantee the reproducibility. So, you know, the name and the kernel that you're using, but you see that there is no resolution key present. There are no requirements, and there is no requirements lock. So something that you can also check on the metadata at the moment, there is nothing. So what you can do with this extension is for example, %horus requirements --add specific package, let's say boto3, for example, numpy, or whatever you prefer. And this automatically adds the requirements to the notebooks. As you see everything that was done with this command, this synced here, and this is also synced on the UI and everything that you want to use, other things, but let's focus on this one. And now that we have the, the dependency, if we do %horus check again, for example, you can see that this one was fixed. So now we have the requirements. We're good. What we need to do is to run , %horus lock. If you want to receive the recommendation or we can run %horus lock as suggested, but we can maybe give a name to the kernel. We can use this flag and say kernel name, demo1. So this first example that we do with this tutorial, and then we just run %horus lock. So what is happening on the background, is basically taking that requirements and is submitting it to the thoth service. And once thoth is providing the resolution, then it'll come back with the block file and all the basic decision on this type of stock. So you will have a list of a table with a list of decision on the recommended stock. So why specific packages have been selected? And once that is done, then as you see here, so the resolution finished and thoth provide with the recommendation. So here you have, specific information and messages related to the packages in general but also specific to the runtime environment. So there was no cuda, this is the CPU family that was identified automatically by the system. And as you see, there are specific packages that are removed for specific reasons. So the user can immediately see why some packages have been basically discarded by the solution engine. This is for the application stock guidance, and they've also recommended the stock report. So information specific to the packages. There are no CBE for boto3, for example, other information coming from  prescription that were collected on specific packages.  And once it's done, it's a little bit slow maybe because we don't have many resources, but later after this is going to install everything that was provided by the resolution. So you see that there is the list of all the packages and all the information and why they were selected. So then it's starting to work on the packages and on the back is using micro pipenv to install them. And then this are the packages that are going to be installed at the end of their resolution. So as you see, the pipfile lock is here and is described, and now it's basically installing the direct dependency and the transitive dependency that I described here. This is done by micro pipenv. And at the end, it just based basically on the kernel that was used the parameters that were used in the resolution and what resolver was used. And as you can see here now together with the requirements, there is also the resolution engine thoth, the requirements lock. So all the information are now stored here, and there is also the thoth configuration file that basically state, what kind of information were used in order to provide that recommendation. So you see also the CPU, the Python version that was used, the recommendation that is used by default. You can of course use the flag to customize the recommendation that you want. And that's basically how you start working on a new notebook. So we basically finish the first part, which is what you saw before. We can also do, %horus --show, yes, that's another command that allows you to see what is inside the notebook. So see that there is the kernel name, the dependencies solution engine, the pipfile, and the pipfile lock. And there should be also the top configuration file, which is described here. So this is what if you want to see, of course, in a better way, not reading this, json, what is inside the notebook. So if there are no question, so I would go ahead to the second example. So let's go to the next one. So the second one case is bring your notebook and make it reproducible. There is a nice feature that is part of a thoth which is able to read the content of the notebook and actually identify what packages are required to run the notebook. So let's go to the other example, discover my notebook. And here there these steps that we are gonna consider basically. So as you see, the notebook has this two cells, which are the actual code, but imagine that you have a notebook that you are bringing from some project that you were making and you want to use now Jupyterlab requirements. So the first thing you do, you can check that this notebook basically is no requirements, and there is a specific command called ,%horus --discover. So when you run %horus --discover, It's going to analyze the content of this notebook and identify what packages needs to be installed. So as you see, the one that was identifies is pandas, which is actually correct, even though there is an import of that package is not using this notebook, as you see only pandas issues, not numpy. So the system is able, the thoth service is able to automatically recognize from the content, what are the actual packages used and everything, as we said, is going to be stored hopefully. Okay. Maybe there is some problem, But basically after that, I think there was an issue that I fixed and the new release is not out yet. But anyway, what you see is basically what we saw before is the same content. So this kind of result is stored in the metadata. There is an issue that I solved, but sorry, it was not in this version. It'll be in the next release, but then you can repeat the same commands. So check that everything is solved. Will we can basically do this automatically now with this command that have an issue, but not from this, from the next version. So, sorry about that. If you have any question, let me know, or if you're stuck somewhere, or if you're having problems, otherwise I will go ahead with the next case.

Speaker1    00:44:16    Francesco if people are working on this, maybe after the talk and they have questions. Is there a good repository or place where they can reach? Yeah.

Speaker2    00:44:28    Yes. So Jupyterlab requirements is the place to go. If you have any problems regarding the resolution, the Jupyter lab requirements, but also if you have trouble with anything in tutorial, you can just open an issue in the URL of tutorial. And here you have the commentation related to the three way to interact with the system. So magic commands is basically what we're doing here. You have all the description of the commands, the option that you can use, what you can do, and you have the same for the other way of working, so if you want to use the UI or if you to use the CLI, so, I mean, let's go ahead to next. Then the next one is convert your notebook and make it reproducible. So now I'm taking a notebook from someone and this notebook has this cells, but as no, we want to basically convert this cells in a command that can give us reproducibility. So what we do, we go to this make notebook reproducible, and we have one command called, %horus convert. So, %horus convert is going to read these cells and convert them in commands that you can run in order to guarantee reproducibility. So if you do, %horus convert, yes, you have also some warnings that comes with it. And I don't know if there is some specific problem at the moment. Maybe it was not present before, so well, could still on the development phase. But the idea is that, these commands will be basically commented out. So they won't be lost. You can still have a look at them. And what you can see is basically the command that we saw before. So the %horus requirements add the specific package, and for these two packages, I really don't know why, what is the moment there were several fixes. So maybe when you repeat it with the new version, it's gonna work correctly. And yes, we can see here, basically the same things that I just described. So I just see that when you run %horus convert, they will be commented out and it will be provided with the same command that can be used with the Jupyter lab requirements. And then you can repeat the same command we saw before. So you just lock, and then you're ready to go. The last example is if someone gives me a notebook, which is reproducible already, what I can do is I can just basically select a specific kernel and run, %horus set kernel, this command, as we see from the documentation can be used. Set kernel can be provided with the one flag. So if you ever read a kernel, you can also override it. So if you're using the same name or you want to be sure that everything is installed properly, then this force will basically delete the kernel that is there and recreate it with the dependencies that are provided from the notebook and can do this even with this example that we created before. So this is basically a notebook, which is reproducible now. And if I give it to someone else, then I can run the command or check to verify that everything is correctly synced. And then I can do, %horus set-kernal and use force if I want to create a new kernel with a new name, and this will do thing, but basically in this moment is just saying that requirement already satisfied because we already have that kernel. Otherwise, if you use the force, it will delete and the redo, well, of course it is a little bit slower, but because he has to recreate the kernel and at the end with this command as you see, now it's already done because everything is set, but basically this is the last example, I think. So they were four example.  There is one more thing that, uh, you can do with this extension. We provided a new command in the menu, which is also something that will be provided in a command in the next release, but you can delete kernels. So if you're creating many kernels and you want to delete them, they're gonna be added here as you see, I have two kernels, I want to delete them. I can just select one and automatically this would be removed and that's all I think, I'm sorry about this two commands. I will verify why it was not working now, but it was working before. So please, if you have any question or any problems with the extension, let me know. We are very happy to receive any contribution as you see, there are a lot of the things that could be done in the extension. So if you also want to explore how to make nice UI, or if you want to provide contribution, it's open, of course, you can just open an issue and say, I want to work on it and we can discuss there, and we will be very happy to work together with you.

Speaker1    00:51:16    Cool. Thanks Francesco and so somebody wants to try this out, like right away using the Elyra experimental image on the small cluster is the way to go.

Speaker2    00:51:29    Yes, there is that specific image that we selected today, which is called the experimental notebook dependencies image. I can also add it to maybe to this slide before sending, I guess we publish also the slide so we can add them there, but the team image is an experimental one. So whenever I release Jupyter lab requirements, I always ask for recreating the image. So you are always provided with the latest version. So if you want to test things, you can also use that image.

Speaker1    00:52:10    Cool. Well, Any questions? Did anyone get a Server or Jupyter lab running in unintending issues? Very cool. Well, thank you so much, Francesco. I really appreciate that. I mean, it's very cool needed tool for sharing and reproducing notebooks and that is I think it's basically like the default way that we manage our dependencies in our group as data scientists.

Speaker2    00:52:57    Thank you Michael and thank you for inviting me.

Speaker1    00:53:01    Of course. So any other questions, if not, we can jump off a couple minutes early here. Awesome. Well, thank you all so much for attending today. If you're here, you probably know how to contact community members in between meetups, but there is slack channel that we use as well as going onto our GitHub repository or GitHub page or the operate first web site are all good places to reach out to us for any questions that you might have. So thanks a lot and talk to y'all soon in a couple weeks.
